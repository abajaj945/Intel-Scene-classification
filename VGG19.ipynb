{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "4kVS7mZAfDu2",
    "outputId": "0cee3d3f-8b8f-4810-af34-5ef53aefe32d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "from keras.callbacks import *\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "#unzip the dataset\n",
    "!unzip 'gdrive/My Drive/train-scene classification.zip'\n",
    "#read the csv file\n",
    "df_raw=pd.read_csv(\"train.csv\")#read csv file\n",
    "\n",
    "#function to load all images and their corresponding labels\n",
    "IMAGES_PATH=\"train/{}\"\n",
    "def load_images_train(data,images_path):\n",
    "    resize=(150,150)\n",
    "    x=[]\n",
    "    y=[]\n",
    "    ids=[]\n",
    "    i=0\n",
    "    for img_id,lbl in data.values:\n",
    "        i=i+1\n",
    "        if(i%1000==0):\n",
    "           print(\"{}/{}\".format(i,len(data)))\n",
    "        image=cv2.imread(images_path.format(img_id))\n",
    "        x.append(cv2.resize(image,resize))\n",
    "        y.append(lbl)\n",
    "        ids.append(img_id)\n",
    "        \n",
    "    return x,y,ids\n",
    "\n",
    "#load data\n",
    "data,labels,ids=load_images_train(df_raw,IMAGES_PATH)\n",
    "\n",
    "#now let's split the dataset into training and validation data\n",
    "X_train,X_test,y_train,y_test=train_test_split(data,labels,test_size=0.30)\n",
    "#prepare the dataset so that it could be feeded to model\n",
    "X_train=np.stack(X_train,axis=0)\n",
    "X_test=np.stack(X_test,axis=0)\n",
    "y_train=to_categorical(y_train,num_classes=6)\n",
    "y_test=to_categorical(y_test,num_classes=6)\n",
    "\n",
    "\n",
    "#create a base model\n",
    "base_model3 = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "NUM_CLASSES=6\n",
    "#freeze all layers in base_model\n",
    "for layers in base_model3.layers:\n",
    "    layers.trainable=False\n",
    "#now stack more layers on the model\n",
    "model3 = Sequential([\n",
    "    base_model3,\n",
    "    Flatten(),\n",
    "    Dense(2048, activation='relu'),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "#create an optimizer\n",
    "optimizer = Adam(0.00001,decay=0.000001)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "#Now let's train the model\n",
    "history=model3.fit(X_train,y_train,batch_size=32,epochs=35,validation_data=(X_test,y_test))\n",
    "\n",
    "y=model3.evaluate(X_test,y_test)\n",
    "\n",
    "test=pd.read_csv(\"gdrive/My Drive/test_WyRytb0.csv\")\n",
    "\n",
    "#function to load all images and their corresponding labels\n",
    "IMAGES_PATH=\"train/{}\"\n",
    "def load_images_test(data,images_path):\n",
    "    resize=(150,150)\n",
    "\n",
    "    x=[]\n",
    "    \n",
    "    ids=[]\n",
    "    i=0\n",
    "    for img_id in data.values:\n",
    "        i=i+1\n",
    "        if(i%1000==0):\n",
    "           print(\"{}/{}\".format(i,len(data)))\n",
    "        image=cv2.imread(images_path.format(img_id[0]))\n",
    "        x.append(cv2.resize(image,resize))\n",
    "       \n",
    "        ids.append(img_id[0])\n",
    "        \n",
    "    return x,ids\n",
    "\n",
    "X_sub=load_images_test(test,IMAGES_PATH)\n",
    "\n",
    "X_sub,names=X_sub\n",
    "\n",
    "X_sub=np.stack(X_sub,axis=0)\n",
    "\n",
    "pred=model3.predict_classes(X_sub,batch_size=32)\n",
    "\n",
    "sub={\"image_name\":names,\"label\":pred.tolist()}\n",
    "\n",
    "dataframe_sub=pd.DataFrame(sub)\n",
    "dataframe_sub.to_csv(\"gdrive/My Drive/submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IU8yoqjan4T7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
